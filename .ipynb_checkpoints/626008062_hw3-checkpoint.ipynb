{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 3:  Embeddings + Recommenders\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, April 9 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main learning objectives: (i) implement and evaluate a pre-cursor to modern word2vec embeddings; and (ii) implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw3.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Donâ€™t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/achadha7/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/achadha7/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the brown corpus and the stopwords\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Corpus words:\n",
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]\n"
     ]
    }
   ],
   "source": [
    "brown_words = brown.words()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Brown Corpus words:\")\n",
    "print(brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'brown_words' (ConcatenatedCorpusView)\n",
      "Stored 'stop_words' (set)\n"
     ]
    }
   ],
   "source": [
    "%store brown_words\n",
    "%store stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable brown_words\n"
     ]
    }
   ],
   "source": [
    "%store -r brown_words\n",
    "%store -r stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation from brown corpus vocab\n",
    "# Convert everything to lower case\n",
    "def preprocess_data(vocab, stop_words):\n",
    "    brown_tokens = [''.join(re.split('\\W+', word.lower())).encode('ascii', 'ignore') for word in vocab if ''.join(re.split('\\W+',word.lower())) not in stop_words]\n",
    "    return brown_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brown corpus length: ', 687794)\n"
     ]
    }
   ],
   "source": [
    "brown_words_cleaned = preprocess_data(brown_words, stop_words)\n",
    "corpus_length = len(brown_words_cleaned)\n",
    "print(\"Brown corpus length: \", corpus_length)\n",
    "\n",
    "fdist = nltk.FreqDist(brown_words_cleaned)\n",
    "fdist.pop(\"\")\n",
    "\n",
    "context = fdist.most_common(1000)\n",
    "vocabulary = fdist.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Top 20 words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292),\n",
       " ('man', 1207),\n",
       " ('even', 1170),\n",
       " ('made', 1125),\n",
       " ('also', 1069),\n",
       " ('many', 1030),\n",
       " ('must', 1013),\n",
       " ('years', 1001),\n",
       " ('af', 996),\n",
       " ('back', 966),\n",
       " ('well', 961)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocabulary Top 20 words:\")\n",
    "vocabulary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Top 10 words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Context Top 10 words:\")\n",
    "context[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a word to integer index map for all the elements present in the list\n",
    "def create_index_map(words):\n",
    "    mmap = {}\n",
    "    for w in words:\n",
    "        mmap[w[0]] = len(mmap)\n",
    "    return mmap\n",
    "\n",
    "vocabulary_to_index_map = create_index_map(vocabulary)\n",
    "context_to_index_map = create_index_map(context)\n",
    "\n",
    "# Creating a tuple of vocabulary and context index maps\n",
    "index_map = (vocabulary_to_index_map, context_to_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depending on the definition of *context*, we can take a document, a paragraph or a sentence as context.\n",
    "For the purpose of this assignment, I am using *entire corpus* for defining context of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use paragraphs for defining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.']\n"
     ]
    }
   ],
   "source": [
    "for word in brown.paras()[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or we can use sentences for defining context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n",
      "any\n",
      "irregularities\n",
      "took\n",
      "place\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in brown.sents()[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the co-occurence matrix between vocabularyu and context words\n",
    "def create_co_occurence_matrix(words, index_map, window_size = 2):\n",
    "    \n",
    "    matrix = np.ones((len(vocabulary), len(context)))\n",
    "\n",
    "    vocabulary_to_index_map = index_map[0]\n",
    "    context_to_index_map = index_map[1]\n",
    "    corpus_length = len(words)\n",
    "    \n",
    "    for i in range(corpus_length):\n",
    "        word1 = words[i]\n",
    "        if word1 not in vocabulary_to_index_map:\n",
    "            continue\n",
    "\n",
    "        w1_id = vocabulary_to_index_map[word1]\n",
    "        start_range = i - window_size\n",
    "        end_range = i + window_size\n",
    "\n",
    "        for j in range(start_range, end_range + 1):\n",
    "            if j >= 0 and j < corpus_length:\n",
    "                word2 = words[j]\n",
    "                if word2 not in context_to_index_map:\n",
    "                    continue\n",
    "                else: \n",
    "                    w2_id = context_to_index_map[word2]\n",
    "                    matrix[w1_id][w2_id] += 1\n",
    "    return matrix\n",
    "\n",
    "co_occurence_matrix = create_co_occurence_matrix(brown_words_cleaned, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a matrix where each cell represents Pr(c|w)\n",
    "# Pr(c|w) = count(c,w)/ count(w)\n",
    "def create_probablity_distribution_matrix(co_occurence_matrix):\n",
    "    pmatrix = np.copy(co_occurence_matrix)\n",
    "    pmatrix = pmatrix/pmatrix.sum(axis = 1)[:, None]\n",
    "    return pmatrix\n",
    "\n",
    "# Returns a vector of probablities of context words Pr(c)\n",
    "def create_probabilities_context_words(co_occurence_matrix):\n",
    "    context_words_counts = co_occurence_matrix.sum(axis = 0)\n",
    "    return context_words_counts/context_words_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Pr(c|w) for each c,w combination in co-occurence matrix\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Pr(c|w) for each c,w combination in co-occurence matrix\")\n",
    "pmatrix = create_probablity_distribution_matrix(co_occurence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Pr(c) for each context word\n",
      "\n",
      "Top 10 context words probabilities:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('one', 0.0027895882916831945),\n",
       "             ('would', 0.0025106811119925956),\n",
       "             ('said', 0.001648306998986293),\n",
       "             ('new', 0.0018566265591255674),\n",
       "             ('could', 0.0018366554277403147),\n",
       "             ('time', 0.0018039440918506766),\n",
       "             ('two', 0.0017047770946273524),\n",
       "             ('may', 0.001714418330468509),\n",
       "             ('first', 0.0016791245206928468),\n",
       "             ('like', 0.0015644626801533783)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "print(\"Calculating Pr(c) for each context word\")\n",
    "print('')\n",
    "print(\"Top 10 context words probabilities:\")\n",
    "context_probablities = create_probabilities_context_words(co_occurence_matrix)[:, None]\n",
    "\n",
    "context_words_prob = OrderedDict(zip([i[0] for i in context[:10]], [i[0] for i in context_probablities[:10]]))\n",
    "context_words_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to divide each column in the probablities matrix with the context word probablities\n",
    "# Pr(c|w)/ Pr(c)\n",
    "\n",
    "ppmi_matrix = np.copy(pmatrix)\n",
    "ppmi_matrix =  (ppmi_matrix.T / context_probablities).T\n",
    "\n",
    "zero_matrix = np.zeros((len(vocabulary), len(context)))\n",
    "\n",
    "ppmi_matrix = np.maximum(zero_matrix, np.log(ppmi_matrix))\n",
    "ppmi_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ppmi_matrix' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.85649903 1.00520118 0.4451773  ... 0.         0.         0.        ]\n",
      " [1.05780658 4.91522082 1.41405286 ... 0.         0.         0.        ]\n",
      " [0.66330886 1.57957902 5.59820389 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.05875109 0.05254632 0.04992557]\n",
      " [0.         0.         0.         ... 0.06165948 0.0554547  0.05283396]\n",
      " [0.         0.         0.         ... 0.07043574 0.06423097 0.06161022]]\n"
     ]
    }
   ],
   "source": [
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vocabulary_map = {v:k for k,v in vocabulary_to_index_map.items()}\n",
    "index_context_map = {v:k for k, v in context_to_index_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kmean_model' (KMeans)\n"
     ]
    }
   ],
   "source": [
    "%store -r ppmi_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "kmean_model = KMeans(n_clusters=100, init='k-means++', max_iter=100)\n",
    "kmean_model.fit(ppmi_matrix)\n",
    "%store kmean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r kmean_model\n",
    "order_centroids = kmean_model.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 10 elements in every cluster that are nearest to centroid\n",
      "\n",
      "Cluster 0:\n",
      "==========\n",
      " 1960  30  1  year  june  1961  fiscal  1959  march  20  last  4 \n",
      "\n",
      "Cluster 1:\n",
      "==========\n",
      " medical  public  department  national  government  forces  program  services  service  state  support  military \n",
      "\n",
      "Cluster 2:\n",
      "==========\n",
      " man  young  old  woman  wife  like  told  children  one  age  men  husband \n",
      "\n",
      "Cluster 3:\n",
      "==========\n",
      " got  ive  never  get  enough  know  done  hes  cold  ever  seen  lot \n",
      "\n",
      "Cluster 4:\n",
      "==========\n",
      " york  new  city  central  times  state  area  england  members  government  council  system \n",
      "\n",
      "Cluster 5:\n",
      "==========\n",
      " used  use  means  power  may  water  pressure  given  data  without  also  much \n",
      "\n",
      "Cluster 6:\n",
      "==========\n",
      " per  cent  million  10  15  20  12  years  100  1  25  average \n",
      "\n",
      "Cluster 7:\n",
      "==========\n",
      " development  research  program  industrial  new  economic  medical  council  programs  technical  planning  effort \n",
      "\n",
      "Cluster 8:\n",
      "==========\n",
      " take  place  care  would  let  time  must  ill  away  could  us  may \n",
      "\n",
      "Cluster 9:\n",
      "==========\n",
      " different  quite  two  ways  things  types  person  kind  used  among  rates  something \n",
      "\n",
      "Cluster 10:\n",
      "==========\n",
      " provide  county  needs  school  may  state  meet  service  plan  court  children  board \n",
      "\n",
      "Cluster 11:\n",
      "==========\n",
      " even  though  cannot  fact  yet  taken  matter  may  one  course  however  perhaps \n",
      "\n",
      "Cluster 12:\n",
      "==========\n",
      " many  must  great  use  certain  fact  upon  may  also  often  important  number \n",
      "\n",
      "Cluster 13:\n",
      "==========\n",
      " year  last  fiscal  per  tax  ago  first  next  1961  past  every  30 \n",
      "\n",
      "Cluster 14:\n",
      "==========\n",
      " found  find  still  way  may  could  always  one  although  difficult  see  much \n",
      "\n",
      "Cluster 15:\n",
      "==========\n",
      " mrs  j  mr  john  e  dr  william  c  r  b  james  brown \n",
      "\n",
      "Cluster 16:\n",
      "==========\n",
      " know  think  dont  didnt  people  would  something  must  youre  much  could  like \n",
      "\n",
      "Cluster 17:\n",
      "==========\n",
      " af  point  p  line  points  image  c  function  since  two  plane  thus \n",
      "\n",
      "Cluster 18:\n",
      "==========\n",
      " around  across  house  back  water  west  away  car  street  along  front  south \n",
      "\n",
      "Cluster 19:\n",
      "==========\n",
      " get  going  sure  thats  enough  dont  think  know  big  right  take  youre \n",
      "\n",
      "Cluster 20:\n",
      "==========\n",
      " policy  foreign  american  united  aid  states  total  economic  countries  officer  soviet  respect \n",
      "\n",
      "Cluster 21:\n",
      "==========\n",
      " administration  food  feed  aid  daily  state  federal  program  first  kennedy  use  foreign \n",
      "\n",
      "Cluster 22:\n",
      "==========\n",
      " go  let  back  far  would  home  want  dont  wanted  could  school  ill \n",
      "\n",
      "Cluster 23:\n",
      "==========\n",
      " couldnt  get  help  see  remember  understand  even  keep  knew  find  write  stop \n",
      "\n",
      "Cluster 24:\n",
      "==========\n",
      " fiscal  year  tax  years  close  june  cities  30  states  1959  end  state \n",
      "\n",
      "Cluster 25:\n",
      "==========\n",
      " remained  day  long  sense  interest  side  four  however  almost  fact  always  american \n",
      "\n",
      "Cluster 26:\n",
      "==========\n",
      " general  shall  secretary  law  moral  order  act  consider  effects  public  state  case \n",
      "\n",
      "Cluster 27:\n",
      "==========\n",
      " af  values  surface  number  temperature  single  form  1  information  p  size  used \n",
      "\n",
      "Cluster 28:\n",
      "==========\n",
      " tax  sales  income  property  year  bill  state  fiscal  due  pay  made  additional \n",
      "\n",
      "Cluster 29:\n",
      "==========\n",
      " going  im  think  tell  sure  youre  get  back  talking  take  know  still \n",
      "\n",
      "Cluster 30:\n",
      "==========\n",
      " time  long  period  ago  enough  take  since  short  first  spent  last  come \n",
      "\n",
      "Cluster 31:\n",
      "==========\n",
      " morning  early  night  next  day  evening  late  home  afternoon  sunday  last  year \n",
      "\n",
      "Cluster 32:\n",
      "==========\n",
      " door  went  back  room  front  hall  across  walked  open  bed  toward  came \n",
      "\n",
      "Cluster 33:\n",
      "==========\n",
      " would  could  never  anything  know  say  like  something  else  nothing  knew  didnt \n",
      "\n",
      "Cluster 34:\n",
      "==========\n",
      " democratic  party  institutions  leaders  committee  county  free  national  faith  among  president  relations \n",
      "\n",
      "Cluster 35:\n",
      "==========\n",
      " director  district  planning  family  recently  greater  carried  washington  main  state  office  division \n",
      "\n",
      "Cluster 36:\n",
      "==========\n",
      " state  states  federal  government  new  united  program  general  president  national  department  development \n",
      "\n",
      "Cluster 37:\n",
      "==========\n",
      " market  stock  price  common  open  place  value  share  american  big  development  general \n",
      "\n",
      "Cluster 38:\n",
      "==========\n",
      " within  days  system  hour  larger  distance  area  group  years  reach  social  year \n",
      "\n",
      "Cluster 39:\n",
      "==========\n",
      " step  toward  first  next  took  taken  every  one  go  second  program  forward \n",
      "\n",
      "Cluster 40:\n",
      "==========\n",
      " would  make  might  made  must  much  little  one  better  work  could  well \n",
      "\n",
      "Cluster 41:\n",
      "==========\n",
      " college  university  students  student  state  president  school  work  chicago  years  education  major \n",
      "\n",
      "Cluster 42:\n",
      "==========\n",
      " small  schools  large  many  business  programs  number  area  training  school  major  private \n",
      "\n",
      "Cluster 43:\n",
      "==========\n",
      " approach  problem  another  price  new  christian  letter  basic  sound  rate  change  question \n",
      "\n",
      "Cluster 44:\n",
      "==========\n",
      " figure  2  shown  3  1  4  based  indicated  12  lines  taken  point \n",
      "\n",
      "Cluster 45:\n",
      "==========\n",
      " good  deal  job  bad  man  one  health  thing  reason  look  old  pretty \n",
      "\n",
      "Cluster 46:\n",
      "==========\n",
      " said  head  toward  took  turned  clay  press  back  came  slowly  started  went \n",
      "\n",
      "Cluster 47:\n",
      "==========\n",
      " physical  education  law  strength  moral  others  social  plant  temperature  knowledge  needs  true \n",
      "\n",
      "Cluster 48:\n",
      "==========\n",
      " six  months  ago  years  five  four  two  spent  three  weeks  several  12 \n",
      "\n",
      "Cluster 49:\n",
      "==========\n",
      " one  first  half  place  time  put  another  second  thing  two  piece  day \n",
      "\n",
      "Cluster 50:\n",
      "==========\n",
      " looked  like  around  saw  back  upon  eyes  face  thought  stopped  straight  see \n",
      "\n",
      "Cluster 51:\n",
      "==========\n",
      " men  women  young  two  four  group  many  come  like  three  knew  children \n",
      "\n",
      "Cluster 52:\n",
      "==========\n",
      " school  high  children  board  college  schools  students  system  feet  still  low  local \n",
      "\n",
      "Cluster 53:\n",
      "==========\n",
      " eyes  white  face  hair  back  red  hands  blue  black  head  room  door \n",
      "\n",
      "Cluster 54:\n",
      "==========\n",
      " stand  could  must  things  look  make  square  alone  side  end  took  back \n",
      "\n",
      "Cluster 55:\n",
      "==========\n",
      " life  human  social  world  society  economic  american  community  history  political  present  experience \n",
      "\n",
      "Cluster 56:\n",
      "==========\n",
      " value  farm  af  great  questions  p  function  market  high  system  products  single \n",
      "\n",
      "Cluster 57:\n",
      "==========\n",
      " death  life  father  rate  history  christ  cause  passed  natural  thus  another  march \n",
      "\n",
      "Cluster 58:\n",
      "==========\n",
      " tax  interest  paid  rate  pay  costs  increase  return  amount  due  money  higher \n",
      "\n",
      "Cluster 59:\n",
      "==========\n",
      " used  also  data  clay  method  methods  word  may  say  words  still  different \n",
      "\n",
      "Cluster 60:\n",
      "==========\n",
      " get  ill  back  could  trying  dont  away  along  going  didnt  able  couldnt \n",
      "\n",
      "Cluster 61:\n",
      "==========\n",
      " island  rhode  state  miss  week  university  interested  long  hospital  property  brown  based \n",
      "\n",
      "Cluster 62:\n",
      "==========\n",
      " around  looked  world  look  corner  turned  walked  go  turn  car  back  walk \n",
      "\n",
      "Cluster 63:\n",
      "==========\n",
      " board  members  committee  member  association  national  meeting  club  university  council  student  country \n",
      "\n",
      "Cluster 64:\n",
      "==========\n",
      " united  states  europe  nations  america  western  among  world  countries  throughout  southern  parts \n",
      "\n",
      "Cluster 65:\n",
      "==========\n",
      " dead  living  man  book  shot  hes  mans  three  center  body  mr  let \n",
      "\n",
      "Cluster 66:\n",
      "==========\n",
      " union  soviet  states  power  members  western  communist  leaders  series  state  meeting  policy \n",
      "\n",
      "Cluster 67:\n",
      "==========\n",
      " available  information  best  data  made  evidence  use  funds  public  also  make  materials \n",
      "\n",
      "Cluster 68:\n",
      "==========\n",
      " wanted  go  know  mother  get  way  see  give  part  money  take  ever \n",
      "\n",
      "Cluster 69:\n",
      "==========\n",
      " asked  question  questions  whether  congress  help  one  board  could  anything  see  anyone \n",
      "\n",
      "Cluster 70:\n",
      "==========\n",
      " possible  make  best  makes  soon  made  much  far  many  also  would  making \n",
      "\n",
      "Cluster 71:\n",
      "==========\n",
      " came  back  voice  home  moment  day  end  time  first  around  sat  later \n",
      "\n",
      "Cluster 72:\n",
      "==========\n",
      " years  ago  two  ten  three  several  hundred  five  four  many  long  six \n",
      "\n",
      "Cluster 73:\n",
      "==========\n",
      " help  couldnt  needed  could  would  us  people  cant  pay  little  cannot  god \n",
      "\n",
      "Cluster 74:\n",
      "==========\n",
      " apparently  decided  simple  feeling  mr  words  experience  although  least  end  mrs  left \n",
      "\n",
      "Cluster 75:\n",
      "==========\n",
      " local  board  state  government  school  church  national  become  level  law  business  training \n",
      "\n",
      "Cluster 76:\n",
      "==========\n",
      " god  son  mother  christ  love  told  born  faith  father  story  wrote  letter \n",
      "\n",
      "Cluster 77:\n",
      "==========\n",
      " 3  6  4  5  7  8  shown  1  figures  2  figure  per \n",
      "\n",
      "Cluster 78:\n",
      "==========\n",
      " want  know  dont  didnt  see  get  go  think  care  tell  much  like \n",
      "\n",
      "Cluster 79:\n",
      "==========\n",
      " federal  state  court  law  district  government  states  tax  funds  cases  board  decision \n",
      "\n",
      "Cluster 80:\n",
      "==========\n",
      " important  thing  perhaps  things  one  certain  role  less  two  difference  part  another \n",
      "\n",
      "Cluster 81:\n",
      "==========\n",
      " according  principle  standard  theory  dr  system  cars  strength  medical  report  surface  plan \n",
      "\n",
      "Cluster 82:\n",
      "==========\n",
      " years  hundred  three  minutes  thousand  five  days  ten  feet  ago  four  two \n",
      "\n",
      "Cluster 83:\n",
      "==========\n",
      " education  higher  public  health  physical  programs  program  schools  board  state  defense  national \n",
      "\n",
      "Cluster 84:\n",
      "==========\n",
      " back  left  right  side  along  away  around  across  car  road  wall  street \n",
      "\n",
      "Cluster 85:\n",
      "==========\n",
      " poet  de  oh  cattle  james  style  playing  caught  glass  heat  charles  thomas \n",
      "\n",
      "Cluster 86:\n",
      "==========\n",
      " labor  secretary  areas  department  relations  supply  costs  union  family  division  movement  issue \n",
      "\n",
      "Cluster 87:\n",
      "==========\n",
      " division  planning  industrial  research  products  steps  labor  continue  activities  army  central  first \n",
      "\n",
      "Cluster 88:\n",
      "==========\n",
      " increased  production  costs  number  volume  population  pressure  per  demand  rates  activity  cause \n",
      "\n",
      "Cluster 89:\n",
      "==========\n",
      " last  night  week  day  one  next  every  first  year  month  days  per \n",
      "\n",
      "Cluster 90:\n",
      "==========\n",
      " eyes  closed  blue  opened  dark  saw  met  looked  open  black  clear  keep \n",
      "\n",
      "Cluster 91:\n",
      "==========\n",
      " company  light  power  union  even  national  officers  east  policy  name  began  john \n",
      "\n",
      "Cluster 92:\n",
      "==========\n",
      " 1  2  war  table  world  figure  class  3  volume  shown  type  st \n",
      "\n",
      "Cluster 93:\n",
      "==========\n",
      " social  economic  political  system  institutions  life  development  science  military  level  problems  aid \n",
      "\n",
      "Cluster 94:\n",
      "==========\n",
      " great  peace  third  gave  corps  time  us  would  world  life  may  opportunity \n",
      "\n",
      "Cluster 95:\n",
      "==========\n",
      " world  war  2  1  since  cold  peace  nuclear  end  free  throughout  second \n",
      "\n",
      "Cluster 96:\n",
      "==========\n",
      " makes  difference  possible  little  man  feel  clear  effort  necessary  real  one  god \n",
      "\n",
      "Cluster 97:\n",
      "==========\n",
      " could  man  got  never  would  like  get  little  much  people  thought  said \n",
      "\n",
      "Cluster 98:\n",
      "==========\n",
      " studies  growth  research  brown  data  recent  basis  show  clay  indicated  made  continue \n",
      "\n",
      "Cluster 99:\n",
      "==========\n",
      " form  forms  information  list  word  af  required  basic  complete  reading  order  available \n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmean_model.cluster_centers_.argsort(axis=1)[:, ::-1]\n",
    "\n",
    "print(\"Printing 12 elements in every cluster that are nearest to centroid\")\n",
    "for i in range(100):\n",
    "    print('')\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    print(\"==========\")\n",
    "    for ind in order_centroids[i, :12]:\n",
    "        print(' %s' % index_vocabulary_map[ind]),\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you see any relation between the words in a cluster?\n",
    "Definitely! Yes.\n",
    "Words in each cluster are very similar to each other. Let's look into few of the examples.\n",
    "1. **_Cluster 0_**:\n",
    "Years (1960, 1959), months(june, march), and dates(30, 4, 20) are clustered together, which in fact almost always occur together.\n",
    "2. **_Cluster 1_**:\n",
    "Words like national, government, program, department, state, service, often occur together while talking about political topics\n",
    "3. **_Cluster 2_**:\n",
    "Man woman, wife, husband, young, old, are the words which often occur together.\n",
    "4. **_Cluster 7_**:\n",
    "Development, researcg, economic, technical, planning are again words which often occur together in economy context.\n",
    "\n",
    "We can find similar patterns throughout the clusters found in the above approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means code reference - \n",
    "https://pythonprogramminglanguage.com/kmeans-text-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours for top-20 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'neighbours_model' (NearestNeighbors)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbours_model = NearestNeighbors(n_neighbors=7, metric = 'cosine')\n",
    "neighbours_model.fit(ppmi_matrix)\n",
    "%store neighbours_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 one\n",
      "===========\n",
      "one another thing day least man good \n",
      "\n",
      "2 would\n",
      "===========\n",
      "would never like say could things let \n",
      "\n",
      "3 said\n",
      "===========\n",
      "said mr skyros hal borden arlene mitchell \n",
      "\n",
      "4 new\n",
      "===========\n",
      "new york yankees city orleans jersey central \n",
      "\n",
      "5 could\n",
      "===========\n",
      "could see never hear would anything way \n",
      "\n",
      "6 time\n",
      "===========\n",
      "time long first period short place spent \n",
      "\n",
      "7 two\n",
      "===========\n",
      "two three ago hundred years weeks four \n",
      "\n",
      "8 may\n",
      "===========\n",
      "may also desirable seem find well might \n",
      "\n",
      "9 first\n",
      "===========\n",
      "first time second last two place day \n",
      "\n",
      "10 like\n",
      "===========\n",
      "like look know felt think would didnt \n",
      "\n",
      "11 man\n",
      "===========\n",
      "man woman old young fat one big \n",
      "\n",
      "12 even\n",
      "===========\n",
      "even though seem perhaps might much know \n",
      "\n",
      "13 made\n",
      "===========\n",
      "made payments payment make mistake clear feel \n",
      "\n",
      "14 also\n",
      "===========\n",
      "also may used must statement readily made \n",
      "\n",
      "15 many\n",
      "===========\n",
      "many people things among times great others \n",
      "\n",
      "16 must\n",
      "===========\n",
      "must therefore able learn plan take something \n",
      "\n",
      "17 years\n",
      "===========\n",
      "years ago months hundred five days ten \n",
      "\n",
      "18 af\n",
      "===========\n",
      "af q zg polynomial function operator tangent \n",
      "\n",
      "19 back\n",
      "===========\n",
      "back went around go home turned come \n",
      "\n",
      "20 well\n",
      "===========\n",
      "well might suited get think known may \n"
     ]
    }
   ],
   "source": [
    "%store -r neighbours_model\n",
    "\n",
    "top_10_nn = neighbours_model.kneighbors(ppmi_matrix[:20,], n_neighbors=7)\n",
    "top_10_nn_indices = top_10_nn[1]\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"\")\n",
    "    print('{0} {1}'.format(str(i+1), index_vocabulary_map[i]))\n",
    "    print(\"===========\")\n",
    "    for j in top_10_nn_indices[i, :]:\n",
    "        print(index_vocabulary_map[j]),\n",
    "    print(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the findings make sense?\n",
    "Out of the top 20 common words nearest neighbours, most of the nearest neighbours do make sense.\n",
    "1. Months, days, ago are nearest neighbours of **years**, representing time of the year.\n",
    "2. Payments, Payment, make are closer to **made** as we often use these the phrase \"make/made payment/payments\".\n",
    "3. Woman, old, young are closer to **man** as we often use phrases like \"young man\", \"man-woman\", \"old woman\".\n",
    "4. second, last, two are closer to **first** representing numbers/ positions.\n",
    "\n",
    "There are some clusters which don't make sense **immediately**.\n",
    "\n",
    "**af -  q zg polynomial function operator tangent**\n",
    "\n",
    "### However,\n",
    "on closer look, we often see mathemetical functions like \n",
    "a(f), q, z(g) used in equations along with words like polynomial, tangent operator.\n",
    "### Hence, even this group make perfect sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- donâ€™t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats.stats import pearsonr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "headers=['MovieID', 'UserID', 'Rating']\n",
    "train_data = pd.read_csv('netflix-dataset/TrainingRatings.txt', names = headers)\n",
    "test_data = pd.read_csv('netflix-dataset/TestingRatings.txt', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Analysis:\n",
      "\n",
      "('Number of ratings in training data: ', 3255352)\n",
      "\n",
      "('Dimensions of user-movie matrix: ', (28978, 1821))\n",
      "('Number of unique users ', 28978)\n",
      "('Number of unique movies ', 1821)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>MovieID</th>\n",
       "      <th>8</th>\n",
       "      <th>28</th>\n",
       "      <th>43</th>\n",
       "      <th>48</th>\n",
       "      <th>61</th>\n",
       "      <th>64</th>\n",
       "      <th>66</th>\n",
       "      <th>92</th>\n",
       "      <th>96</th>\n",
       "      <th>111</th>\n",
       "      <th>...</th>\n",
       "      <th>17654</th>\n",
       "      <th>17660</th>\n",
       "      <th>17689</th>\n",
       "      <th>17693</th>\n",
       "      <th>17706</th>\n",
       "      <th>17725</th>\n",
       "      <th>17728</th>\n",
       "      <th>17734</th>\n",
       "      <th>17741</th>\n",
       "      <th>17742</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "MovieID  8      28     43     48     61     64     66     92     96     111    \\\n",
       "UserID                                                                          \n",
       "7          5.0    4.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "79         NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "199        NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    4.0   \n",
       "481        NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    5.0   \n",
       "769        NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "906        NaN    3.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1310       NaN    3.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1333       3.0    2.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    2.0   \n",
       "1427       NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1442       NaN    4.0    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "MovieID  ...    17654  17660  17689  17693  17706  17725  17728  17734  17741  \\\n",
       "UserID   ...                                                                    \n",
       "7        ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "79       ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "199      ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "481      ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "769      ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "906      ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1310     ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1333     ...      NaN    NaN    NaN    2.0    NaN    3.0    NaN    NaN    NaN   \n",
       "1427     ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1442     ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "MovieID  17742  \n",
       "UserID          \n",
       "7          NaN  \n",
       "79         NaN  \n",
       "199        NaN  \n",
       "481        NaN  \n",
       "769        NaN  \n",
       "906        NaN  \n",
       "1310       NaN  \n",
       "1333       NaN  \n",
       "1427       NaN  \n",
       "1442       NaN  \n",
       "\n",
       "[10 rows x 1821 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the number of ratings in training data\n",
    "print(\"Training data Analysis:\")\n",
    "print(\"\")\n",
    "number_of_ratings = len(train_data)\n",
    "print(\"Number of ratings in training data: \", number_of_ratings)\n",
    "\n",
    "# Create a user-movie matrix from the input training examples\n",
    "ratings_df = train_data.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "\n",
    "print(\"\")\n",
    "print(\"Dimensions of user-movie matrix: \", ratings_df.shape)\n",
    "print(\"Number of unique users \", ratings_df.shape[0])\n",
    "print(\"Number of unique movies \", ratings_df.shape[1])\n",
    "ratings_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data Analysis:\n",
      "\n",
      "('Number of ratings in training data: ', 100478)\n",
      "\n",
      "('Dimensions of user-movie matrix: ', (27555, 1701))\n",
      "('Number of unique users ', 27555)\n",
      "('Number of unique movies ', 1701)\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of ratings in training data\n",
    "print(\"Testing data Analysis:\")\n",
    "print(\"\")\n",
    "number_of_ratings_test = len(test_data)\n",
    "print(\"Number of ratings in training data: \",number_of_ratings_test)\n",
    "\n",
    "# Create a user-movie matrix from the input training examples\n",
    "test_ratings_df = test_data.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "\n",
    "print(\"\")\n",
    "print(\"Dimensions of user-movie matrix: \", test_ratings_df.shape)\n",
    "print(\"Number of unique users \", test_ratings_df.shape[0])\n",
    "print(\"Number of unique movies \", test_ratings_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling NA with 0 values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>MovieID</th>\n",
       "      <th>8</th>\n",
       "      <th>28</th>\n",
       "      <th>43</th>\n",
       "      <th>48</th>\n",
       "      <th>61</th>\n",
       "      <th>64</th>\n",
       "      <th>66</th>\n",
       "      <th>92</th>\n",
       "      <th>96</th>\n",
       "      <th>111</th>\n",
       "      <th>...</th>\n",
       "      <th>17654</th>\n",
       "      <th>17660</th>\n",
       "      <th>17689</th>\n",
       "      <th>17693</th>\n",
       "      <th>17706</th>\n",
       "      <th>17725</th>\n",
       "      <th>17728</th>\n",
       "      <th>17734</th>\n",
       "      <th>17741</th>\n",
       "      <th>17742</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "MovieID  8      28     43     48     61     64     66     92     96     111    \\\n",
       "UserID                                                                          \n",
       "7          5.0    4.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "79         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    4.0   \n",
       "481        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    5.0   \n",
       "769        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "906        0.0    3.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1310       0.0    3.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1333       3.0    2.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    2.0   \n",
       "1427       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1442       0.0    4.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "MovieID  ...    17654  17660  17689  17693  17706  17725  17728  17734  17741  \\\n",
       "UserID   ...                                                                    \n",
       "7        ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "79       ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199      ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "481      ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "769      ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "906      ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1310     ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1333     ...      0.0    0.0    0.0    2.0    0.0    3.0    0.0    0.0    0.0   \n",
       "1427     ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1442     ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "MovieID  17742  \n",
       "UserID          \n",
       "7          0.0  \n",
       "79         0.0  \n",
       "199        0.0  \n",
       "481        0.0  \n",
       "769        0.0  \n",
       "906        0.0  \n",
       "1310       0.0  \n",
       "1333       0.0  \n",
       "1427       0.0  \n",
       "1442       0.0  \n",
       "\n",
       "[10 rows x 1821 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gives the mean value of ratings for every user\n",
    "mean_user_ratings = ratings_df.mean(axis=1, skipna=True)\n",
    "\n",
    "print(\"Filling NA with 0 values\")\n",
    "ratings_df.fillna(0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = ratings_df.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/achadha7/.conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.516012623367705\n"
     ]
    }
   ],
   "source": [
    "# Given a user, movie pair, this method will return the predicted ratings of the given user\n",
    "def predict(user, movie, ratings_df, mean_user_ratings, K = 0.001):\n",
    "    input_user_ratings = ratings_df.loc[user].fillna(0)\n",
    "        \n",
    "    # Dataframe of all the users who rated the movie\n",
    "    users_who_rated_movie_df = ratings_df[~ ratings_df[movie].isnull()].fillna(0)\n",
    "    #users_who_rated_movie_df = users_who_rated_movie_df.drop(index=user)\n",
    "    \n",
    "    # Average rating of the input user (v_a)\n",
    "    user_avg_rating = mean_user_ratings.loc[user]\n",
    "    \n",
    "    # Difference between the movie rating and average rating given by user\n",
    "    movie_avg_rating_diff = users_who_rated_movie_df[movie] - mean_user_ratings.ix[users_who_rated_movie_df.index]\n",
    "    correlation = [ pearsonr(a, input_user_ratings)[0] for a in users_who_rated_movie_df.values]\n",
    "    prediction = user_avg_rating + K * np.sum(correlation * movie_avg_rating_diff)\n",
    "    return prediction\n",
    "\n",
    "def predict_test_user(row):\n",
    "    return predict(row['UserID'], row['MovieID'], ratings_df, mean_user_ratings)\n",
    "\n",
    "# Testing the above function\n",
    "print(predict(7, 8, ratings_df, mean_user_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for 10 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>573364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.136290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2149668</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.118801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1089184</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.562902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2465894</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.778449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>534508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.780150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>992921</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.887262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>595054</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.827784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1298304</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.827333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1661600</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.412832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>553787</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID   UserID  Rating  predictions\n",
       "0        8   573364     1.0     3.136290\n",
       "1        8  2149668     3.0     3.118801\n",
       "2        8  1089184     3.0     2.562902\n",
       "3        8  2465894     3.0     2.778449\n",
       "4        8   534508     1.0     2.780150\n",
       "5        8   992921     4.0     2.887262\n",
       "6        8   595054     4.0     2.827784\n",
       "7        8  1298304     4.0     3.827333\n",
       "8        8  1661600     4.0     3.412832\n",
       "9        8   553787     2.0     3.000888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the predictions for test data set\n",
    "test_data['predictions'] = test_data.head(100).apply(predict_test_user, axis=1)\n",
    "print(\"Getting the predictions for first 100 entries in test data\")\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for 1000 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the predictions for first 1000 entries in test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>156</td>\n",
       "      <td>905964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.500318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>156</td>\n",
       "      <td>2387369</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.517594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>156</td>\n",
       "      <td>1941703</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.481254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>156</td>\n",
       "      <td>525071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.331513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>156</td>\n",
       "      <td>1824009</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.497851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>156</td>\n",
       "      <td>2572409</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.528752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>156</td>\n",
       "      <td>2086711</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.018688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>156</td>\n",
       "      <td>869468</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.930133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>156</td>\n",
       "      <td>942513</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.026905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>156</td>\n",
       "      <td>203369</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.738591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MovieID   UserID  Rating  predictions\n",
       "990      156   905964     1.0     3.500318\n",
       "991      156  2387369     3.0     3.517594\n",
       "992      156  1941703     3.0     3.481254\n",
       "993      156   525071     1.0     2.331513\n",
       "994      156  1824009     4.0     3.497851\n",
       "995      156  2572409     3.0     3.528752\n",
       "996      156  2086711     3.0     4.018688\n",
       "997      156   869468     4.0     2.930133\n",
       "998      156   942513     5.0     4.026905\n",
       "999      156   203369     4.0     3.738591"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the predictions for test data set\n",
    "test_data['predictions'] = test_data.head(1000).apply(predict_test_user, axis=1)\n",
    "print(\"Getting the predictions for first 1000 entries in test data\")\n",
    "test_data[990:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the predictions for test data set\n",
    "test_data['predictions'] = test_data.head(5000).apply(predict_test_user, axis=1)\n",
    "%store test_data\n",
    "print(\"Getting the predictions for first 5000 entries in test data\")\n",
    "print(\"Printing the last 10 rows\")\n",
    "test_data[4990:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>573364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.136290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2149668</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.118801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1089184</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.562902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2465894</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.778449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>534508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.780150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>992921</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.887262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>595054</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.827784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1298304</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.827333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1661600</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.412832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>553787</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID   UserID  Rating  predictions\n",
       "0        8   573364     1.0     3.136290\n",
       "1        8  2149668     3.0     3.118801\n",
       "2        8  1089184     3.0     2.562902\n",
       "3        8  2465894     3.0     2.778449\n",
       "4        8   534508     1.0     2.780150\n",
       "5        8   992921     4.0     2.887262\n",
       "6        8   595054     4.0     2.827784\n",
       "7        8  1298304     4.0     3.827333\n",
       "8        8  1661600     4.0     3.412832\n",
       "9        8   553787     2.0     3.000888"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_test_matrix = test_data.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean Absolute Error: ', 0.9181966999819647)\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "MAE = np.mean(np.abs(test_data['predictions'] - test_data['Rating']))\n",
    "print(\"Mean Absolute Error: \", MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Root Mean Squared Error: ', 0.10435823995555074)\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "RMSE = np.sqrt(np.mean(test_data['predictions'] - test_data['Rating']) **2)\n",
    "print(\"Root Mean Squared Error: \", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the users ratings\n",
    "Normalise the user-ratings matrix by subtracting user's mean from every value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = ratings_df.fillna(0).as_matrix()\n",
    "normalized_ratings = ratings_matrix - mean_user_ratings.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Latent Factor Models\n",
    "Using simple SVD approach and using 35 latent factors (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "U, Sigma, Vt = svds(normalised_ratings_df, k=35)\n",
    "Sigma = np.diag(Sigma)\n",
    "predicted_ratings_np = np.dot(np.dot(U, Sigma), Vt) + mean_user_ratings.values.reshape(-1, 1)\n",
    "predicted_ratings_df = pd.DataFrame(predicted_ratings_np, columns=normalised_ratings_df.columns, index=normalised_ratings_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean Absolute Error: ', 1.9609625311939223)\n",
      "('Root Mean Squared Error: ', 1.8941864039326723)\n"
     ]
    }
   ],
   "source": [
    "def prediction_ratings(test_row):\n",
    "    return predicted_ratings_df[test_row['MovieID']].loc[test_row['UserID']]\n",
    "\n",
    "test_data['predictions'] = test_data.apply(prediction_ratings, axis=1)\n",
    "\n",
    "# Mean Absolute Error\n",
    "MAE = np.mean(np.abs(test_data['predictions'] - test_data['Rating']))\n",
    "print(\"Mean Absolute Error: \", MAE)\n",
    "# Root Mean Squared Error\n",
    "RMSE = np.sqrt(np.mean(test_data['predictions'] - test_data['Rating']) **2)\n",
    "print(\"Root Mean Squared Error: \", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "Using K =35 with SVD didn't do any improvement on the previous approach using person corelation.\n",
    "* One reason might be that number of latent factors in this case are very small and on increasing the value of K, RMSE and MAE may reduce.\n",
    "* Another reason could be that since we are filling the missing values with 0, this is leading to erroneous behaviour in the recommendation system as it will interpret 0 as a poor rating.\n",
    "Fixing above two points might improve on the pearson correlation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Matrix Factorization along with regularization\n",
    "Using gradient descent approach to find P and Q factors which can represent user and movie feature matrix without loosing much information.\n",
    "We are using regularization to prevent overfitting using L2 norm.\n",
    "\n",
    "### For K = 50, users = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_regularized(X,P,Q,K,steps,alpha,beta):\n",
    "    Q = Q.T\n",
    "    m = X.shape[0]\n",
    "    for step in xrange(steps):\n",
    "        #for each user\n",
    "        for i in xrange(X.shape[0]):\n",
    "           \n",
    "            #for each item\n",
    "            for j in xrange(X.shape[1]):\n",
    "                if X[i][j] > 0 :\n",
    "\n",
    "                    #calculate the error of the element\n",
    "                    eij = X[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    #second norm of P and Q for regularilization\n",
    "                    sum_of_norms = 0\n",
    "                    sum_of_norms += LA.norm(P) + LA.norm(Q)\n",
    "                    #print sum_of_norms\n",
    "                    eij += ((beta/2) * sum_of_norms)\n",
    "                    #print eij\n",
    "                    #compute the gradient from the error\n",
    "                    for k in xrange(K):\n",
    "                        P[i][k] = P[i][k] + alpha * ( 2 * eij * Q[k][j] - (beta * P[i][k]))\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - (beta * Q[k][j]))\n",
    "\n",
    "        #compute total error\n",
    "        error = 0\n",
    "        #for each user\n",
    "        for i in xrange(X.shape[0]):\n",
    "            #for each item\n",
    "            for j in xrange(X.shape[1]):\n",
    "                if X[i][j] > 0:\n",
    "                    error += np.power(X[i][j] - np.dot(P[i,:],Q[:,j]),2)\n",
    "        if error/m < 0.001:\n",
    "            break\n",
    "        if step%10==0:\n",
    "            print(\"Step {0}: RMSE is {1}\".format(step, np.sqrt(error/m)))\n",
    "        \n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/achadha7/.conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: RMSE is 48.2097336818\n",
      "Step 10: RMSE is 15.1383623112\n",
      "Step 20: RMSE is 14.8301863914\n",
      "Step 30: RMSE is 14.69837685\n",
      "Step 40: RMSE is 14.6006746974\n",
      "Step 50: RMSE is 14.525450095\n",
      "Step 60: RMSE is 14.4661281139\n",
      "Step 70: RMSE is 14.4183805716\n",
      "Step 80: RMSE is 14.3792542299\n",
      "Step 90: RMSE is 14.346677663\n",
      "Step 100: RMSE is 14.3191636161\n",
      "Step 110: RMSE is 14.2956225662\n",
      "Step 120: RMSE is 14.2752422592\n",
      "Step 130: RMSE is 14.2574077579\n",
      "Step 140: RMSE is 14.2416471135\n",
      "Step 150: RMSE is 14.2275936723\n",
      "Step 160: RMSE is 14.2149594338\n",
      "Step 170: RMSE is 14.2035159043\n",
      "Step 180: RMSE is 14.1930801287\n",
      "Step 190: RMSE is 14.1835043625\n"
     ]
    }
   ],
   "source": [
    "small_ratings_matrix = ratings_df.head(20).fillna(0).as_matrix()\n",
    "small_normalized_ratings = small_ratings_matrix - mean_user_ratings.head(20).reshape(-1,1)\n",
    "users_count = small_ratings_matrix.shape[0]\n",
    "movies_count = small_ratings_matrix.shape[1]\n",
    "\n",
    "K = 50\n",
    "\n",
    "U = np.random.rand(users_count, K)\n",
    "V = np.random.rand(movies_count, K)\n",
    "steps = 200\n",
    "alpha = 0.0005\n",
    "beta = float(0.02)\n",
    "\n",
    "U_new, V_new = matrix_factorization_regularized(small_normalized_ratings, U, V, K, steps, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: RMSE is 106.402794342\n",
      "Step 10: RMSE is 22.3487612787\n",
      "Step 20: RMSE is 15.2308604616\n",
      "Step 30: RMSE is 14.3145791237\n",
      "Step 40: RMSE is 14.172106066\n",
      "Step 50: RMSE is 14.1449634201\n",
      "Step 60: RMSE is 14.1376727475\n",
      "Step 70: RMSE is 14.1343197459\n",
      "Step 80: RMSE is 14.1319518856\n",
      "Step 90: RMSE is 14.1299599929\n",
      "Step 100: RMSE is 14.1281947089\n",
      "Step 110: RMSE is 14.126604399\n",
      "Step 120: RMSE is 14.1251610341\n",
      "Step 130: RMSE is 14.1238440824\n",
      "Step 140: RMSE is 14.1226366535\n",
      "Step 150: RMSE is 14.1215243517\n",
      "Step 160: RMSE is 14.1204947728\n",
      "Step 170: RMSE is 14.1195371839\n",
      "Step 180: RMSE is 14.1186422769\n",
      "Step 190: RMSE is 14.1178019643\n",
      "Step 200: RMSE is 14.117009209\n",
      "Step 210: RMSE is 14.1162578788\n",
      "Step 220: RMSE is 14.1155426231\n",
      "Step 230: RMSE is 14.1148587679\n",
      "Step 240: RMSE is 14.1142022263\n",
      "Step 250: RMSE is 14.1135694211\n",
      "Step 260: RMSE is 14.1129572192\n",
      "Step 270: RMSE is 14.1123628748\n",
      "Step 280: RMSE is 14.1117839808\n",
      "Step 290: RMSE is 14.111218426\n"
     ]
    }
   ],
   "source": [
    "K=45\n",
    "steps = 300\n",
    "alpha = 0.0002\n",
    "beta = float(0.02)\n",
    "\n",
    "U_new, V_new = matrix_factorization_regularized(small_normalized_ratings, U, V, K, steps, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "Above results are for just for 20 users , with iterations < 300.\n",
    "Using these paramters we are able to achieve RMSE ~ 14 which is still way more than what we achieved in the simple pearson correlation technique.\n",
    "\n",
    "If we use this technique for users more than 20, say 1000, and run it for more number of iteratiuons, we may get better RMSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Approach - Belkor's Approach\n",
    "### ENSEMBLE!!\n",
    "\n",
    "In the last approach, we can merge all the various models we obtained in the previous steps, and give final prediction for the user's ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "https://beckernick.github.io/matrix-factorization-recommender/\n",
    "http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
